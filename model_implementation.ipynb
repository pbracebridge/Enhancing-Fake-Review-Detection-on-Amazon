{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ef8fb7-6aa4-46e0-abf6-55cb8e1a09fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "\n",
    "# General Libraries for numerical computations and data handling\n",
    "import numpy as np  # Numerical operations\n",
    "import pandas as pd  # Data manipulation and analysis\n",
    "import torch  # PyTorch framework for deep learning\n",
    "\n",
    "# Machine Learning & Feature Selection\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV  # Data splitting and hyperparameter tuning\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler  # Feature scaling\n",
    "from sklearn.ensemble import RandomForestClassifier  # Random Forest classifier\n",
    "from sklearn.linear_model import LogisticRegression  # Logistic Regression model\n",
    "from sklearn.svm import SVC  # Support Vector Classifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix  # Model evaluation metrics\n",
    "\n",
    "# Deep Learning (GNN & RNN)\n",
    "import torch.nn as nn  # Neural network components in PyTorch\n",
    "import torch.optim as optim  # Optimization algorithms for training\n",
    "from torch_geometric.data import Data  # Graph data structure for GNNs\n",
    "from torch_geometric.nn import GCNConv, SAGEConv  # GNN layers (Graph Convolutional Network & GraphSAGE)\n",
    "\n",
    "# Recurrent Neural Network (RNN) with Keras\n",
    "from keras.models import Sequential  # Sequential API for building neural networks\n",
    "from keras.layers import LSTM, Dense, Bidirectional, Dropout  # LSTM-based architecture layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6414fc54-1f47-4da9-8837-d056f0f86c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "file_path = \"/Users/patrick/Desktop/Dissertation/Fake review project/data/reviews_with_features.csv\"\n",
    "reviews_dataset = pd.read_csv(file_path)\n",
    "\n",
    "# Define Feature Groups for Different Models\n",
    "\n",
    "# Random Forest (RF) Features\n",
    "rf_features = ['category', 'text_length_x_readability', 'text_length',\n",
    "               'readability', 'length_sentiment_ratio', 'sentiment', 'log_text_length'\n",
    "]\n",
    "\n",
    "# Logistic Regression (LR) Features\n",
    "lr_features = ['category', 'rating', 'text_length_x_readability', 'text_length', \n",
    "               'readability', 'length_sentiment_ratio', 'avg_rating', 'sentiment', \n",
    "               'rating_deviation'\n",
    "]\n",
    "\n",
    "# Support Vector Machine (SVM) Features\n",
    "svm_features = ['category', 'rating', 'text_length_x_readability', 'text_length', \n",
    "                'readability', 'length_sentiment_ratio', 'sentiment', \n",
    "                'rating_deviation', 'log_text_length'\n",
    "]\n",
    "\n",
    "# Graph Neural Network (GNN) Features (Group-based)\n",
    "gnn_features = [\"category\", \"rolling_review_count\"\n",
    "]\n",
    "\n",
    "# Recurrent Neural Network (RNN) Features (Temporal-based)\n",
    "rnn_features = ['rolling_review_count', 'rolling_rating_mean', 'days_since_last_review'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48bd410-f7e8-4f24-b91a-e841fbff0e49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Logistic Regression Model with Hyperparameter Tuning\n",
    "print(\"Training Logistic Regression with Hyperparameter Tuning...\")\n",
    "\n",
    "# Standardize feature values to ensure uniform scale\n",
    "scaler_lr = StandardScaler()\n",
    "X_lr = scaler_lr.fit_transform(reviews_dataset[lr_features].fillna(0))  # Fill missing values with 0\n",
    "y = reviews_dataset['label']  # Target variable\n",
    "\n",
    "# Split dataset into training and testing sets (80/20 split, stratified by label)\n",
    "X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(\n",
    "    X_lr, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Define hyperparameter grid for tuning\n",
    "param_grid = {'C': [0.01, 0.1, 1, 10, 100], 'solver': ['lbfgs', 'liblinear']}\n",
    "\n",
    "# Perform grid search with cross-validation (5-fold)\n",
    "grid_search = GridSearchCV(LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced'), param_grid, cv=5)\n",
    "\n",
    "# Train the model using the best hyperparameter combination\n",
    "grid_search.fit(X_train_lr, y_train_lr)\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Retrieve the best model from grid search\n",
    "best_lr_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_lr = best_lr_model.predict(X_test_lr)\n",
    "\n",
    "# Evaluate model performance\n",
    "accuracy = accuracy_score(y_test_lr, y_pred_lr)\n",
    "conf_matrix = confusion_matrix(y_test_lr, y_pred_lr)\n",
    "\n",
    "# Display evaluation metrics\n",
    "print(f\"Logistic Regression Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_lr, y_pred_lr))\n",
    "print(f\"False Positives: {conf_matrix[0,1]}, False Negatives: {conf_matrix[1,0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb170271-1eb2-43ee-8611-fc0fb16defd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Model\n",
    "print(\"Training Random Forest Model...\")\n",
    "\n",
    "# Standardize feature values to ensure consistent scaling\n",
    "scaler_rf = StandardScaler()\n",
    "X_rf = scaler_rf.fit_transform(reviews_dataset[rf_features])  # Use predefined RF features\n",
    "y = reviews_dataset[\"label\"]  # Target variable\n",
    "\n",
    "# Split dataset into training and testing sets (80/20 split, stratified by label)\n",
    "X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(\n",
    "    X_rf, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Initialize and train the Random Forest model with 100 estimators\n",
    "rf_model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "rf_model.fit(X_train_rf, y_train_rf)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_rf = rf_model.predict(X_test_rf)\n",
    "\n",
    "# Evaluate model performance\n",
    "rf_accuracy = accuracy_score(y_test_rf, y_pred_rf)\n",
    "conf_matrix_rf = confusion_matrix(y_test_rf, y_pred_rf)  # Compute confusion matrix\n",
    "\n",
    "# Display evaluation results\n",
    "print(\"Random Forest Training Completed.\")\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy:.4f}\")\n",
    "print(\"\\nClassification Report (Random Forest):\")\n",
    "print(classification_report(y_test_rf, y_pred_rf))\n",
    "print(f\"False Positives: {conf_matrix_rf[0,1]}, False Negatives: {conf_matrix_rf[1,0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb5a5a6-00d2-47dc-8cdf-a52f297fd1de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Support Vector Machine (SVM) Model\n",
    "print(\"Training Support Vector Machine (SVM) Model...\")\n",
    "\n",
    "# Standardize feature values for consistent scaling\n",
    "scaler_svm = StandardScaler()\n",
    "X_svm = scaler_svm.fit_transform(reviews_dataset[svm_features])  # Use predefined SVM features\n",
    "y = reviews_dataset[\"label\"]  # Target variable\n",
    "\n",
    "# Split dataset into training and testing sets (80/20 split, stratified by label)\n",
    "X_train_svm, X_test_svm, y_train_svm, y_test_svm = train_test_split(\n",
    "    X_svm, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Initialize and train the SVM model using an RBF kernel\n",
    "svm_model = SVC(kernel=\"rbf\", random_state=42, probability=True)  # Enables probability estimates\n",
    "svm_model.fit(X_train_svm, y_train_svm)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_svm = svm_model.predict(X_test_svm)\n",
    "\n",
    "# Evaluate model performance\n",
    "svm_accuracy = accuracy_score(y_test_svm, y_pred_svm)\n",
    "conf_matrix_svm = confusion_matrix(y_test_svm, y_pred_svm)  # Compute confusion matrix\n",
    "\n",
    "# Display evaluation results\n",
    "print(\"SVM Training Completed.\")\n",
    "print(f\"SVM Model Accuracy: {svm_accuracy:.4f}\")\n",
    "print(\"\\nClassification Report (SVM):\")\n",
    "print(classification_report(y_test_svm, y_pred_svm))\n",
    "print(f\"False Positives: {conf_matrix_svm[0,1]}, False Negatives: {conf_matrix_svm[1,0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63a0d9d-7598-4b81-9682-63617a6a6af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Graph Data for Graph Neural Network (GNN)\n",
    "print(\"Preparing Graph Data for GNN...\")\n",
    "\n",
    "# Ensure 'user_id' column exists; if missing, create a unique ID for each row\n",
    "if 'user_id' not in reviews_dataset.columns:\n",
    "    total_users = len(reviews_dataset)\n",
    "    reviews_dataset['user_id'] = np.arange(total_users)\n",
    "\n",
    "# Construct edge index (user_id â†’ category) for the graph structure\n",
    "edge_index_array = np.vstack([\n",
    "    reviews_dataset['user_id'].values,\n",
    "    reviews_dataset['category'].values\n",
    "])\n",
    "edge_index = torch.tensor(edge_index_array, dtype=torch.long)\n",
    "\n",
    "# Normalize numerical features for GNN training\n",
    "scaler_gnn = MinMaxScaler()\n",
    "numerical_gnn_features = ['rolling_review_count', 'rolling_rating_mean']\n",
    "reviews_dataset[numerical_gnn_features] = scaler_gnn.fit_transform(reviews_dataset[numerical_gnn_features])\n",
    "\n",
    "# Convert features and labels to PyTorch tensors\n",
    "node_features = torch.tensor(reviews_dataset[gnn_features].values, dtype=torch.float)\n",
    "labels = torch.tensor(reviews_dataset['label'].values, dtype=torch.long)\n",
    "\n",
    "# Create a PyTorch Geometric Data object for GNN processing\n",
    "data = Data(x=node_features, edge_index=edge_index, y=labels)\n",
    "\n",
    "# Debugging output\n",
    "print(\"GNN Data Preparation Completed.\")\n",
    "print(f\"Number of Nodes: {data.num_nodes}\")\n",
    "print(f\"Number of Edges: {data.num_edges}\")\n",
    "print(f\"Node Feature Matrix Shape: {data.x.shape}\")\n",
    "print(f\"Labels Shape: {data.y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f7c5dd-50e0-4aaf-bfc9-210c00bd57c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Graph Neural Network (GNN) Model\n",
    "print(\"\\nInitializing GNN Model...\")\n",
    "\n",
    "class EnhancedGNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(EnhancedGNNModel, self).__init__()\n",
    "        self.conv1 = SAGEConv(input_dim, hidden_dim)  # First graph convolution layer\n",
    "        self.conv2 = SAGEConv(hidden_dim, hidden_dim // 2)  # Second graph convolution layer\n",
    "        self.conv3 = SAGEConv(hidden_dim // 2, output_dim)  # Output layer\n",
    "        self.dropout = nn.Dropout(0.4)  # Dropout for regularization\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = torch.relu(self.conv1(x, edge_index))  # Apply ReLU activation after first layer\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.conv2(x, edge_index))  # Apply ReLU activation after second layer\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv3(x, edge_index)  # Node-level output\n",
    "        return x\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "hidden_dim = 64\n",
    "gnn_model = EnhancedGNNModel(input_dim=node_features.size(1), hidden_dim=hidden_dim, output_dim=2)\n",
    "optimizer_gnn = optim.Adam(gnn_model.parameters(), lr=0.01, weight_decay=1e-4)  # Adam optimizer with weight decay\n",
    "loss_function = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
    "\n",
    "print(\"GNN Model Initialized.\")\n",
    "\n",
    "# Train the GNN Model\n",
    "print(\"\\nTraining GNN Model...\")\n",
    "gnn_model.train()\n",
    "for epoch in range(100):  # Train for 100 epochs\n",
    "    optimizer_gnn.zero_grad()\n",
    "    gnn_output = gnn_model(data)  # Forward pass\n",
    "    loss_gnn = loss_function(gnn_output, data.y)  # Compute loss\n",
    "    loss_gnn.backward()  # Backpropagation\n",
    "    optimizer_gnn.step()  # Update weights\n",
    "\n",
    "    # Print progress every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        gnn_predictions = gnn_output.argmax(dim=1).detach().cpu().numpy()\n",
    "        gnn_accuracy = accuracy_score(data.y.cpu().numpy(), gnn_predictions)\n",
    "        print(f\"Epoch {epoch + 1}: Loss = {loss_gnn.item():.4f}, Accuracy = {gnn_accuracy:.4f}\")\n",
    "\n",
    "print(\"GNN Training Completed.\")\n",
    "\n",
    "# Extract GNN Embeddings for Hybrid Model\n",
    "print(\"\\nExtracting GNN Embeddings...\")\n",
    "gnn_model.eval()\n",
    "with torch.no_grad():\n",
    "    gnn_embeddings = gnn_model(data).detach().cpu().numpy()  # Convert tensor to NumPy array\n",
    "\n",
    "print(\"GNN Embeddings Extracted.\")\n",
    "\n",
    "# Evaluate the trained GNN Model\n",
    "print(\"\\nEvaluating GNN Model...\")\n",
    "gnn_model.eval()\n",
    "with torch.no_grad():\n",
    "    gnn_output = gnn_model(data)\n",
    "    gnn_predictions = gnn_output.argmax(dim=1).cpu().numpy()  # Convert tensor to NumPy array\n",
    "    true_labels = data.y.cpu().numpy()  # Extract true labels\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, gnn_predictions)\n",
    "\n",
    "# Extract false positives and false negatives\n",
    "false_positives = conf_matrix[0, 1]  # Misclassified as positive\n",
    "false_negatives = conf_matrix[1, 0]  # Misclassified as negative\n",
    "\n",
    "# Print final evaluation results\n",
    "gnn_accuracy = accuracy_score(true_labels, gnn_predictions)\n",
    "print(f\"GNN Model Accuracy: {gnn_accuracy:.4f}\")\n",
    "print(\"\\nGNN Classification Report:\")\n",
    "print(classification_report(true_labels, gnn_predictions))\n",
    "print(f\"\\nFalse Positives: {false_positives}, False Negatives: {false_negatives}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d301150c-046f-4840-9f21-1febb573196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Sequential Data for Recurrent Neural Network (RNN)\n",
    "print(\"\\nPreparing Sequential Data for RNN...\")\n",
    "\n",
    "# Select features used for the RNN model\n",
    "rnn_features = ['spike_day_reviewers', 'rolling_review_count']\n",
    "\n",
    "# Normalize only the RNN-related features\n",
    "scaler_rnn = MinMaxScaler()\n",
    "temporal_data = scaler_rnn.fit_transform(reviews_dataset[rnn_features])\n",
    "\n",
    "# Function to create sequences of fixed length for RNN training\n",
    "def create_sequences(data, labels, seq_length=10):\n",
    "    sequences, seq_labels = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        sequences.append(data[i:i + seq_length])  # Extract sequence of features\n",
    "        seq_labels.append(labels[i + seq_length])  # Assign label from last time step\n",
    "    return np.array(sequences), np.array(seq_labels)\n",
    "\n",
    "# Generate sequences for RNN model input\n",
    "rnn_input, rnn_labels = create_sequences(temporal_data, reviews_dataset['label'].values, seq_length=10)\n",
    "\n",
    "# Print shape information for verification\n",
    "print(f\"RNN Input Shape: {rnn_input.shape}\")\n",
    "print(f\"RNN Labels Shape: {rnn_labels.shape}\")\n",
    "print(\"RNN Data Preparation Completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21967770-2219-4f00-a3d2-122bacdd0faa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define and Train Optimized Recurrent Neural Network (RNN) Model\n",
    "print(\"\\nInitializing RNN Model...\")\n",
    "\n",
    "from keras.layers import Input\n",
    "\n",
    "# Define the RNN model with an explicit input layer\n",
    "rnn_model = Sequential([\n",
    "    Input(shape=(rnn_input.shape[1], rnn_input.shape[2])),  # Input layer defining sequence length and features\n",
    "    Bidirectional(LSTM(64, return_sequences=True, recurrent_dropout=0.2)),  # First bidirectional LSTM layer\n",
    "    LSTM(32, recurrent_dropout=0.2),  # Second LSTM layer\n",
    "    Dropout(0.3),  # Dropout for regularization\n",
    "    Dense(32, activation='relu'),  # Fully connected layer with ReLU activation\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model using Adam optimizer and binary cross-entropy loss\n",
    "rnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(\"RNN Model Initialized.\")\n",
    "\n",
    "# Train the RNN model\n",
    "print(\"\\nTraining RNN Model...\")\n",
    "rnn_model.fit(rnn_input, rnn_labels, epochs=15, batch_size=32, verbose=1)\n",
    "print(\"RNN Training Completed.\")\n",
    "\n",
    "# Extract RNN Embeddings for Hybrid Model\n",
    "print(\"\\nExtracting RNN Embeddings...\")\n",
    "rnn_embeddings = rnn_model.predict(rnn_input)\n",
    "print(\"RNN Embeddings Extracted.\")\n",
    "\n",
    "# Evaluate RNN Model\n",
    "print(\"\\nEvaluating RNN Model...\")\n",
    "\n",
    "# Ensure labels have the correct shape\n",
    "rnn_labels = rnn_labels.reshape(-1)  # Flatten labels array\n",
    "rnn_predictions = (rnn_embeddings > 0.5).astype(int).flatten()  # Convert probability outputs to binary predictions\n",
    "\n",
    "# Compute accuracy score\n",
    "rnn_accuracy = accuracy_score(rnn_labels, rnn_predictions)\n",
    "\n",
    "# Display classification results\n",
    "print(f\"RNN Model Accuracy: {rnn_accuracy:.4f}\")\n",
    "print(\"\\nRNN Classification Report:\")\n",
    "print(classification_report(rnn_labels, rnn_predictions))\n",
    "\n",
    "# Compute False Positives & False Negatives\n",
    "false_positives = np.sum((rnn_predictions == 1) & (rnn_labels == 0))  # Incorrectly predicted as positive\n",
    "false_negatives = np.sum((rnn_predictions == 0) & (rnn_labels == 1))  # Incorrectly predicted as negative\n",
    "\n",
    "print(f\"False Positives: {false_positives}, False Negatives: {false_negatives}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6932202f-4aac-4d5a-95c1-9ca1ae0f1dbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LOGISTIC REGRESSION HYBRID MODEL\n",
    "\n",
    "# Ensure feature alignment by truncating datasets to the smallest available length\n",
    "min_length = min(len(X_lr), len(gnn_embeddings), len(rnn_embeddings), len(y)) \n",
    "\n",
    "# Truncate datasets to ensure consistency\n",
    "X_lr = X_lr[:min_length]  # Logistic Regression features\n",
    "gnn_embeddings = gnn_embeddings[:min_length]\n",
    "rnn_embeddings = rnn_embeddings[:min_length]\n",
    "labels = torch.tensor(y[:min_length].values, dtype=torch.float).view(-1)\n",
    "\n",
    "print(\"\\nInitializing LR Hybrid Model...\")\n",
    "print(\"LR Hybrid Model Initialized.\")\n",
    "\n",
    "# Normalize features using MinMax scaling\n",
    "scaler = MinMaxScaler()\n",
    "X_combined = np.hstack([X_lr, gnn_embeddings, rnn_embeddings])\n",
    "X_combined_normalized = scaler.fit_transform(X_combined)\n",
    "\n",
    "# Compute interaction features between different components\n",
    "X_lr_exp = np.expand_dims(X_lr, axis=2)  \n",
    "gnn_exp = np.expand_dims(gnn_embeddings, axis=1)  \n",
    "rnn_exp = np.expand_dims(rnn_embeddings, axis=1)  \n",
    "\n",
    "lr_gnn_interaction = (X_lr_exp * gnn_exp).reshape(X_lr.shape[0], -1)  \n",
    "lr_rnn_interaction = (X_lr_exp * rnn_exp).reshape(X_lr.shape[0], -1)  \n",
    "gnn_rnn_interaction = (gnn_exp * rnn_exp).reshape(X_lr.shape[0], -1)  \n",
    "\n",
    "# Combine interaction features with normalized inputs\n",
    "interaction_features = np.hstack([lr_gnn_interaction, lr_rnn_interaction, gnn_rnn_interaction])\n",
    "X_combined_normalized = np.hstack([X_combined_normalized, interaction_features])\n",
    "\n",
    "# Train-Test Split\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "    torch.tensor(X_combined_normalized, dtype=torch.float),\n",
    "    labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Define LR Hybrid Model\n",
    "class LRHybridModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LRHybridModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.bn1(x)\n",
    "\n",
    "        x = self.leaky_relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        x = self.leaky_relu(self.fc3(x))\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        x = torch.sigmoid(self.fc4(x))  # Binary classification output\n",
    "        return x\n",
    "\n",
    "# Initialize LR Hybrid Model\n",
    "hybrid_model = LRHybridModel(input_dim=X_combined_normalized.shape[1])\n",
    "optimizer_hybrid = optim.Adam(hybrid_model.parameters(), lr=0.002, betas=(0.9, 0.99), eps=1e-8, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate scheduler to reduce learning rate when validation loss plateaus\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer_hybrid, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "# Define Focal Loss function for handling class imbalance\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = nn.BCELoss()(inputs, targets)\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "loss_fn = FocalLoss()\n",
    "\n",
    "# Early stopping parameters\n",
    "best_accuracy = 0\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "\n",
    "# Train the LR Hybrid Model\n",
    "print(\"\\nTraining LR Hybrid Model...\")\n",
    "for epoch in range(100):\n",
    "    hybrid_model.train()\n",
    "    optimizer_hybrid.zero_grad()\n",
    "    \n",
    "    predictions = hybrid_model(train_features).squeeze()\n",
    "    loss_hybrid = loss_fn(predictions, train_labels)\n",
    "    \n",
    "    loss_hybrid.backward()\n",
    "    optimizer_hybrid.step()\n",
    "    scheduler.step(loss_hybrid)\n",
    "\n",
    "    # Evaluate model on test set\n",
    "    hybrid_model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_predictions = hybrid_model(test_features).squeeze()\n",
    "        test_predictions_binary = (test_predictions > 0.5).int()\n",
    "        test_accuracy = accuracy_score(test_labels.numpy(), test_predictions_binary.numpy())\n",
    "\n",
    "    # Print training progress every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch + 1}: Loss = {loss_hybrid.item():.4f}, Accuracy = {test_accuracy:.4f}\")\n",
    "\n",
    "    # Implement early stopping\n",
    "    if test_accuracy > best_accuracy:\n",
    "        best_accuracy = test_accuracy\n",
    "        patience_counter = 0\n",
    "        torch.save(hybrid_model.state_dict(), \"best_lr_hybrid_model.pth\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "print(\"LR Hybrid Training Completed.\")\n",
    "\n",
    "# Load the best model checkpoint\n",
    "hybrid_model.load_state_dict(torch.load(\"best_lr_hybrid_model.pth\", weights_only=True))\n",
    "\n",
    "# Final Model Evaluation\n",
    "hybrid_model.eval()\n",
    "with torch.no_grad():\n",
    "    final_predictions = hybrid_model(test_features).squeeze()\n",
    "    final_predictions_binary = (final_predictions > 0.5).int()\n",
    "\n",
    "# Compute Confusion Matrix\n",
    "conf_matrix = confusion_matrix(test_labels.numpy(), final_predictions_binary.numpy())\n",
    "false_positives = conf_matrix[0, 1]  # Incorrectly classified as positive\n",
    "false_negatives = conf_matrix[1, 0]  # Incorrectly classified as negative\n",
    "\n",
    "# Evaluate final model performance\n",
    "lr_hybrid_best_accuracy = accuracy_score(test_labels.numpy(), final_predictions_binary.numpy())\n",
    "\n",
    "# Print final evaluation results\n",
    "print(\"\\nExtracting LR Hybrid Model Predictions...\")\n",
    "print(\"RF Hybrid Model Predictions Extracted.\")  # Possible typo? Should it be LR Hybrid?\n",
    "\n",
    "print(\"\\nEvaluating LR Hybrid Model...\")\n",
    "print(f\"LR Hybrid Model Accuracy: {lr_hybrid_best_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nLR Hybrid Model Classification Report:\")\n",
    "print(classification_report(test_labels.numpy(), final_predictions_binary.numpy(), zero_division=0))\n",
    "\n",
    "print(f\"\\nFalse Positives: {false_positives}, False Negatives: {false_negatives}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9f2d95-b49a-4798-b1d7-932c062999f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM FOREST HYBRID MODEL\n",
    "\n",
    "# Ensure feature alignment by truncating datasets to the smallest available length\n",
    "min_length = min(len(X_rf), len(gnn_embeddings), len(rnn_embeddings), len(y)) \n",
    "\n",
    "# Truncate datasets to maintain consistency\n",
    "X_rf = X_rf[:min_length]\n",
    "gnn_embeddings = gnn_embeddings[:min_length]\n",
    "rnn_embeddings = rnn_embeddings[:min_length]\n",
    "labels = torch.tensor(y[:min_length].values, dtype=torch.float).view(-1)\n",
    "\n",
    "print(\"\\nInitializing RF Hybrid Model...\")\n",
    "print(\"RF Hybrid Model Initialized.\")\n",
    "\n",
    "# Normalize features using MinMax scaling\n",
    "scaler = MinMaxScaler()\n",
    "X_combined = np.hstack([X_rf, gnn_embeddings, rnn_embeddings])\n",
    "X_combined_normalized = scaler.fit_transform(X_combined)\n",
    "\n",
    "# Compute interaction features between different feature sets\n",
    "X_rf_exp = np.expand_dims(X_rf, axis=2)  \n",
    "gnn_exp = np.expand_dims(gnn_embeddings, axis=1)  \n",
    "rnn_exp = np.expand_dims(rnn_embeddings, axis=1)  \n",
    "\n",
    "rf_gnn_interaction = (X_rf_exp * gnn_exp).reshape(X_rf.shape[0], -1)  \n",
    "rf_rnn_interaction = (X_rf_exp * rnn_exp).reshape(X_rf.shape[0], -1)  \n",
    "gnn_rnn_interaction = (gnn_exp * rnn_exp).reshape(X_rf.shape[0], -1)  \n",
    "\n",
    "# Combine interaction features with normalized inputs\n",
    "interaction_features = np.hstack([rf_gnn_interaction, rf_rnn_interaction, gnn_rnn_interaction])\n",
    "X_combined_normalized = np.hstack([X_combined_normalized, interaction_features])\n",
    "\n",
    "# Train-Test Split\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "    torch.tensor(X_combined_normalized, dtype=torch.float),\n",
    "    labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Define RF Hybrid Model\n",
    "class RFHybridModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(RFHybridModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.bn1(x)\n",
    "\n",
    "        x = self.leaky_relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        x = self.leaky_relu(self.fc3(x))\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        x = torch.sigmoid(self.fc4(x))  # Binary classification output\n",
    "        return x\n",
    "\n",
    "# Initialize RF Hybrid Model\n",
    "hybrid_model = RFHybridModel(input_dim=X_combined_normalized.shape[1])\n",
    "optimizer_hybrid = optim.Adam(hybrid_model.parameters(), lr=0.002, betas=(0.9, 0.99), eps=1e-8, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate scheduler to reduce learning rate when validation loss plateaus\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer_hybrid, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "# Define Focal Loss function for handling class imbalance\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = nn.BCELoss()(inputs, targets)\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "loss_fn = FocalLoss()\n",
    "\n",
    "# Early stopping parameters\n",
    "best_accuracy = 0\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "\n",
    "# Train the RF Hybrid Model\n",
    "print(\"\\nTraining RF Hybrid Model...\")\n",
    "for epoch in range(100):\n",
    "    hybrid_model.train()\n",
    "    optimizer_hybrid.zero_grad()\n",
    "    \n",
    "    predictions = hybrid_model(train_features).squeeze()\n",
    "    loss_hybrid = loss_fn(predictions, train_labels)\n",
    "    \n",
    "    loss_hybrid.backward()\n",
    "    optimizer_hybrid.step()\n",
    "    scheduler.step(loss_hybrid)\n",
    "\n",
    "    # Evaluate model on test set\n",
    "    hybrid_model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_predictions = hybrid_model(test_features).squeeze()\n",
    "        test_predictions_binary = (test_predictions > 0.5).int()\n",
    "        test_accuracy = accuracy_score(test_labels.numpy(), test_predictions_binary.numpy())\n",
    "\n",
    "    # Print training progress every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch + 1}: Loss = {loss_hybrid.item():.4f}, Accuracy = {test_accuracy:.4f}\")\n",
    "\n",
    "    # Implement early stopping\n",
    "    if test_accuracy > best_accuracy:\n",
    "        best_accuracy = test_accuracy\n",
    "        patience_counter = 0\n",
    "        torch.save(hybrid_model.state_dict(), \"best_rf_hybrid_model.pth\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "print(\"RF Hybrid Training Completed.\")\n",
    "\n",
    "# Load the best model checkpoint\n",
    "hybrid_model.load_state_dict(torch.load(\"best_rf_hybrid_model.pth\", weights_only=True))\n",
    "\n",
    "# Final Model Evaluation\n",
    "hybrid_model.eval()\n",
    "with torch.no_grad():\n",
    "    final_predictions = hybrid_model(test_features).squeeze()\n",
    "    final_predictions_binary = (final_predictions > 0.5).int()\n",
    "\n",
    "# Compute Confusion Matrix\n",
    "conf_matrix = confusion_matrix(test_labels.numpy(), final_predictions_binary.numpy())\n",
    "false_positives = conf_matrix[0, 1]  # Incorrectly classified as positive\n",
    "false_negatives = conf_matrix[1, 0]  # Incorrectly classified as negative\n",
    "\n",
    "# Evaluate final model performance\n",
    "rf_hybrid_best_accuracy = accuracy_score(test_labels.numpy(), final_predictions_binary.numpy())\n",
    "\n",
    "# Print final evaluation results\n",
    "print(\"\\nExtracting RF Hybrid Model Predictions...\")\n",
    "print(\"RF Hybrid Model Predictions Extracted.\")\n",
    "\n",
    "print(\"\\nEvaluating RF Hybrid Model...\")\n",
    "print(f\"RF Hybrid Model Accuracy: {rf_hybrid_best_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nRF Hybrid Model Classification Report:\")\n",
    "print(classification_report(test_labels.numpy(), final_predictions_binary.numpy(), zero_division=0))\n",
    "\n",
    "print(f\"\\nFalse Positives: {false_positives}, False Negatives: {false_negatives}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3823da24-165c-43ad-bcb5-091679c0b92c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SVM HYBRID MODEL\n",
    "\n",
    "# Train SVM model and extract probability-based features\n",
    "scaler_svm = MinMaxScaler()\n",
    "X_svm = scaler_svm.fit_transform(reviews_dataset[svm_features])  # Normalize SVM-specific features\n",
    "y = reviews_dataset['label']  # Target variable\n",
    "\n",
    "# Split dataset into training and testing sets for SVM\n",
    "X_train_svm, X_test_svm, y_train_svm, y_test_svm = train_test_split(\n",
    "    X_svm, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train SVM model with an RBF kernel\n",
    "svm_model = SVC(kernel='rbf', probability=True, random_state=42)  # Enable probability predictions\n",
    "svm_model.fit(X_train_svm, y_train_svm)\n",
    "\n",
    "# Extract SVM probability scores to be used as features in the hybrid model\n",
    "svm_probabilities = svm_model.predict_proba(X_svm)[:, 1].reshape(-1, 1)  # Use probability of class 1\n",
    "\n",
    "# Ensure feature alignment by truncating datasets to the smallest available length\n",
    "min_length = min(len(svm_probabilities), len(gnn_embeddings), len(rnn_embeddings), len(y)) \n",
    "\n",
    "# Truncate datasets to maintain consistency\n",
    "svm_probabilities = svm_probabilities[:min_length]\n",
    "gnn_embeddings = gnn_embeddings[:min_length]\n",
    "rnn_embeddings = rnn_embeddings[:min_length]\n",
    "labels = torch.tensor(y[:min_length].values, dtype=torch.float).view(-1)\n",
    "\n",
    "print(\"\\nInitializing SVM Hybrid Model...\")\n",
    "print(\"SVM Hybrid Model Initialized.\")\n",
    "\n",
    "# Normalize combined feature set\n",
    "scaler = MinMaxScaler()\n",
    "X_combined = np.hstack([svm_probabilities, gnn_embeddings, rnn_embeddings])\n",
    "X_combined_normalized = scaler.fit_transform(X_combined)\n",
    "\n",
    "# Compute interaction features between different components\n",
    "svm_exp = np.expand_dims(svm_probabilities, axis=2)  \n",
    "gnn_exp = np.expand_dims(gnn_embeddings, axis=1)  \n",
    "rnn_exp = np.expand_dims(rnn_embeddings, axis=1)  \n",
    "\n",
    "svm_gnn_interaction = (svm_exp * gnn_exp).reshape(svm_probabilities.shape[0], -1)  \n",
    "svm_rnn_interaction = (svm_exp * rnn_exp).reshape(svm_probabilities.shape[0], -1)  \n",
    "gnn_rnn_interaction = (gnn_exp * rnn_exp).reshape(svm_probabilities.shape[0], -1)  \n",
    "\n",
    "# Combine interaction features with normalized input features\n",
    "interaction_features = np.hstack([svm_gnn_interaction, svm_rnn_interaction, gnn_rnn_interaction])\n",
    "X_combined_normalized = np.hstack([X_combined_normalized, interaction_features])\n",
    "\n",
    "# Train-Test Split\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "    torch.tensor(X_combined_normalized, dtype=torch.float),\n",
    "    labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Define SVM Hybrid Model\n",
    "class SVMHybridModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SVMHybridModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.bn1(x)\n",
    "\n",
    "        x = self.leaky_relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        x = self.leaky_relu(self.fc3(x))\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        x = torch.sigmoid(self.fc4(x))  # Binary classification output\n",
    "        return x\n",
    "\n",
    "# Initialize SVM Hybrid Model\n",
    "hybrid_model = SVMHybridModel(input_dim=X_combined_normalized.shape[1])\n",
    "optimizer_hybrid = optim.Adam(hybrid_model.parameters(), lr=0.002, betas=(0.9, 0.99), eps=1e-8, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate scheduler to reduce learning rate when validation loss plateaus\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer_hybrid, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "# Define Focal Loss function for handling class imbalance\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = nn.BCELoss()(inputs, targets)\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "loss_fn = FocalLoss()\n",
    "\n",
    "# Early stopping parameters\n",
    "best_accuracy = 0\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "\n",
    "# Train the SVM Hybrid Model\n",
    "print(\"\\nTraining SVM Hybrid Model...\")\n",
    "for epoch in range(100):\n",
    "    hybrid_model.train()\n",
    "    optimizer_hybrid.zero_grad()\n",
    "    \n",
    "    predictions = hybrid_model(train_features).squeeze()\n",
    "    loss_hybrid = loss_fn(predictions, train_labels)\n",
    "    \n",
    "    loss_hybrid.backward()\n",
    "    optimizer_hybrid.step()\n",
    "    scheduler.step(loss_hybrid)\n",
    "\n",
    "    # Evaluate model on test set\n",
    "    hybrid_model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_predictions = hybrid_model(test_features).squeeze()\n",
    "        test_predictions_binary = (test_predictions > 0.5).int()\n",
    "        test_accuracy = accuracy_score(test_labels.numpy(), test_predictions_binary.numpy())\n",
    "\n",
    "    # Print training progress every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch + 1}: Loss = {loss_hybrid.item():.4f}, Accuracy = {test_accuracy:.4f}\")\n",
    "\n",
    "    # Implement early stopping\n",
    "    if test_accuracy > best_accuracy:\n",
    "        best_accuracy = test_accuracy\n",
    "        patience_counter = 0\n",
    "        torch.save(hybrid_model.state_dict(), \"best_svm_hybrid_model.pth\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "print(\"SVM Hybrid Training Completed.\")\n",
    "\n",
    "# Load the best model checkpoint\n",
    "hybrid_model.load_state_dict(torch.load(\"best_svm_hybrid_model.pth\", weights_only=True))\n",
    "\n",
    "# Final Model Evaluation\n",
    "hybrid_model.eval()\n",
    "with torch.no_grad():\n",
    "    final_predictions = hybrid_model(test_features).squeeze()\n",
    "    final_predictions_binary = (final_predictions > 0.5).int()\n",
    "\n",
    "# Compute Confusion Matrix\n",
    "conf_matrix = confusion_matrix(test_labels.numpy(), final_predictions_binary.numpy())\n",
    "false_positives = conf_matrix[0, 1]  # Incorrectly classified as positive\n",
    "false_negatives = conf_matrix[1, 0]  # Incorrectly classified as negative\n",
    "\n",
    "# Evaluate final model performance\n",
    "svm_hybrid_best_accuracy = accuracy_score(test_labels.numpy(), final_predictions_binary.numpy())\n",
    "\n",
    "# Print final evaluation results\n",
    "print(\"\\nExtracting SVM Hybrid Model Predictions...\")\n",
    "print(\"SVM Hybrid Model Predictions Extracted.\")\n",
    "\n",
    "print(\"\\nEvaluating SVM Hybrid Model...\")\n",
    "print(f\"SVM Hybrid Model Accuracy: {svm_hybrid_best_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nSVM Hybrid Model Classification Report:\")\n",
    "print(classification_report(test_labels.numpy(), final_predictions_binary.numpy(), zero_division=0))\n",
    "\n",
    "print(f\"\\nFalse Positives: {false_positives}, False Negatives: {false_negatives}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee83108f-8590-4b1d-ae7d-a57d84d780b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Accuracy Results\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_score(y_test_lr, y_pred_lr):.4f}\")\n",
    "print(f\"Random Forest Accuracy: {accuracy_score(y_test_rf, y_pred_rf):.4f}\")\n",
    "print(f\"SVM Accuracy: {accuracy_score(y_test_svm, y_pred_svm):.4f}\")\n",
    "print(f\"GNN Accuracy: {gnn_accuracy:.4f}\")\n",
    "print(f\"RNN Accuracy: {rnn_accuracy:.4f}\")\n",
    "print(f\"LR Hybrid Model Accuracy: {lr_hybrid_best_accuracy:.4f}\")\n",
    "print(f\"RF Hybrid Model Accuracy: {rf_hybrid_best_accuracy:.4f}\")\n",
    "print(f\"SVM Hybrid Model Accuracy: {svm_hybrid_best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8e328d-24e1-485d-8f05-ecadab43a717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc  # Garbage collection\n",
    "\n",
    "# Logistic Regression (LR)\n",
    "gc.collect()\n",
    "start_time_lr = time.time()\n",
    "best_lr_model.fit(X_train_lr, y_train_lr)\n",
    "end_time_lr = time.time()\n",
    "lr_training_time = end_time_lr - start_time_lr\n",
    "\n",
    "# Random Forest (RF)\n",
    "gc.collect()\n",
    "start_time_rf = time.time()\n",
    "rf_model.fit(X_train_rf, y_train_rf)\n",
    "end_time_rf = time.time()\n",
    "rf_training_time = end_time_rf - start_time_rf\n",
    "\n",
    "# Support Vector Machine (SVM)\n",
    "gc.collect()\n",
    "start_time_svm = time.time()\n",
    "svm_model.fit(X_train_svm, y_train_svm)\n",
    "end_time_svm = time.time()\n",
    "svm_training_time = end_time_svm - start_time_svm\n",
    "\n",
    "# Graph Neural Network (GNN)\n",
    "gc.collect()\n",
    "start_time_gnn = time.time()\n",
    "for epoch in range(100):  # Training loop\n",
    "    optimizer_gnn.zero_grad()\n",
    "    gnn_output = gnn_model(data)\n",
    "    loss_gnn = loss_function(gnn_output, data.y)\n",
    "    loss_gnn.backward()\n",
    "    optimizer_gnn.step()\n",
    "end_time_gnn = time.time()\n",
    "gnn_training_time = end_time_gnn - start_time_gnn\n",
    "\n",
    "# Recurrent Neural Network (RNN)\n",
    "gc.collect()\n",
    "start_time_rnn = time.time()\n",
    "rnn_model.fit(rnn_input, rnn_labels, epochs=50, batch_size=64, verbose=1)  # Ensure full training\n",
    "end_time_rnn = time.time()\n",
    "rnn_training_time = end_time_rnn - start_time_rnn\n",
    "\n",
    "# Hybrid Models (LR, RF, SVM Hybrid)\n",
    "gc.collect()\n",
    "start_time_lr_hybrid = time.time()\n",
    "lr_hybrid_training_time = train_hybrid_model(\n",
    "    LRHybridModel(input_dim=X_combined_normalized.shape[1]),\n",
    "    optimizer_hybrid, scheduler, train_features, train_labels,\n",
    "    \"LR Hybrid Model\", \"best_lr_hybrid_model.pth\", loss_fn\n",
    ")\n",
    "end_time_lr_hybrid = time.time()\n",
    "lr_hybrid_training_time = end_time_lr_hybrid - start_time_lr_hybrid\n",
    "\n",
    "gc.collect()\n",
    "start_time_rf_hybrid = time.time()\n",
    "rf_hybrid_training_time = train_hybrid_model(\n",
    "    RFHybridModel(input_dim=X_combined_normalized.shape[1]),\n",
    "    optimizer_hybrid, scheduler, train_features, train_labels,\n",
    "    \"RF Hybrid Model\", \"best_rf_hybrid_model.pth\", loss_fn\n",
    ")\n",
    "end_time_rf_hybrid = time.time()\n",
    "rf_hybrid_training_time = end_time_rf_hybrid - start_time_rf_hybrid\n",
    "\n",
    "gc.collect()\n",
    "start_time_svm_hybrid = time.time()\n",
    "svm_hybrid_training_time = train_hybrid_model(\n",
    "    SVMHybridModel(input_dim=X_combined_normalized.shape[1]),\n",
    "    optimizer_hybrid, scheduler, train_features, train_labels,\n",
    "    \"SVM Hybrid Model\", \"best_svm_hybrid_model.pth\", loss_fn\n",
    ")\n",
    "end_time_svm_hybrid = time.time()\n",
    "svm_hybrid_training_time = end_time_svm_hybrid - start_time_svm_hybrid\n",
    "\n",
    "# Print Summary of Training Times\n",
    "print(\"\\n Summary of Training Times for All Models:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Logistic Regression: {lr_training_time:.2f} seconds\")\n",
    "print(f\"Random Forest: {rf_training_time:.2f} seconds\")\n",
    "print(f\"SVM: {svm_training_time:.2f} seconds\")\n",
    "print(f\"GNN: {gnn_training_time:.2f} seconds\")\n",
    "print(f\"RNN: {rnn_training_time:.2f} seconds\")\n",
    "print(f\"LR Hybrid Model: {lr_hybrid_training_time:.2f} seconds\")\n",
    "print(f\"RF Hybrid Model: {rf_hybrid_training_time:.2f} seconds\")\n",
    "print(f\"SVM Hybrid Model: {svm_hybrid_training_time:.2f} seconds\")\n",
    "print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7787841d-cb49-4f7f-894c-9142fdc51490",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
