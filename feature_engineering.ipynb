{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff461102-0189-42c1-b19a-c0435ba8965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for data processing, analysis, and graph operations\n",
    "import pandas as pd  # Data manipulation and analysis\n",
    "import numpy as np  # Numerical computations\n",
    "import networkx as nx  # Graph-based analysis and network operations\n",
    "from textblob import TextBlob  # Sentiment analysis and text processing\n",
    "import textstat  # Readability and text complexity metrics\n",
    "from datetime import timedelta  # Time-based calculations\n",
    "\n",
    "# Load the preprocessed dataset\n",
    "file_path = 'data/reviews_dataset_cleaned.csv'\n",
    "reviews_dataset = pd.read_csv(file_path)\n",
    "\n",
    "print(\"Dataset loaded successfully.\")  # Confirm successful loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73b5d2a-4238-4eac-8fbf-04e48e3b9208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text-Based Feature Engineering\n",
    "\n",
    "# Calculate the number of words in each review\n",
    "reviews_dataset['text_length'] = reviews_dataset['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Ensure the 'cleaned_text' column contains only valid strings (fill NaN values with an empty string)\n",
    "reviews_dataset['cleaned_text'] = reviews_dataset['cleaned_text'].fillna('').astype(str)\n",
    "\n",
    "# Compute sentiment polarity score using TextBlob\n",
    "reviews_dataset['sentiment'] = reviews_dataset['cleaned_text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# Compute readability score using the Flesch-Kincaid grade level\n",
    "try:\n",
    "    reviews_dataset['readability'] = reviews_dataset['text'].apply(lambda x: textstat.flesch_kincaid_grade(x))\n",
    "except Exception as e:\n",
    "    print(f\"Error in readability calculation: {e}\")\n",
    "    reviews_dataset['readability'] = None  # Assign None if an error occurs\n",
    "\n",
    "# Create an interaction feature by multiplying text length with readability score\n",
    "reviews_dataset['text_length_x_readability'] = reviews_dataset['text_length'] * reviews_dataset['readability']\n",
    "\n",
    "# Apply log transformation to text length for better feature scaling\n",
    "reviews_dataset['log_text_length'] = np.log1p(reviews_dataset['text_length'])\n",
    "\n",
    "# Compute the ratio of text length to sentiment score (adding 1 to avoid division by zero)\n",
    "reviews_dataset['length_sentiment_ratio'] = reviews_dataset['text_length'] / (np.abs(reviews_dataset['sentiment']) + 1)\n",
    "\n",
    "print(\"Text-based feature engineering completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6785bf6f-61ce-4cdc-9ed5-417b2ce46b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal Feature Engineering\n",
    "\n",
    "# Convert the 'date' column to datetime format, handling any errors by coercing invalid values to NaT\n",
    "reviews_dataset['date'] = pd.to_datetime(reviews_dataset['date'], errors='coerce')\n",
    "\n",
    "# Compute the number of days since the user's last review\n",
    "reviews_dataset['days_since_last_review'] = reviews_dataset.groupby('user_id')['date'].diff().dt.days.fillna(0)\n",
    "\n",
    "# Extract key temporal features\n",
    "reviews_dataset['is_weekend'] = reviews_dataset['date'].dt.dayofweek >= 5  # Boolean flag: True if the review was posted on a weekend\n",
    "reviews_dataset['month'] = reviews_dataset['date'].dt.month  # Extract the month from the review date\n",
    "\n",
    "# Identify spike days based on daily review volume\n",
    "daily_review_counts = reviews_dataset.groupby('date').size().reset_index(name='review_count')  # Count reviews per day\n",
    "spike_threshold = daily_review_counts['review_count'].mean() + 2 * daily_review_counts['review_count'].std()  # Define threshold for spike days\n",
    "spike_days = daily_review_counts[daily_review_counts['review_count'] > spike_threshold]['date']  # Identify spike days\n",
    "\n",
    "# Compute the number of unique reviewers per day (spike-related feature)\n",
    "reviews_dataset['spike_day_reviewers'] = reviews_dataset.groupby('date')['user_id'].transform('nunique')\n",
    "\n",
    "print(\"Temporal feature engineering completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab823cae-ee1a-44a9-9c6f-d8e212d434ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling Averages and Count-Based Features\n",
    "\n",
    "# Compute the rolling count of reviews per user over the last 7 days\n",
    "reviews_dataset['rolling_review_count'] = (\n",
    "    reviews_dataset.groupby('user_id')['date']\n",
    "    .transform(lambda x: x.diff().dt.days.rolling(window=7, min_periods=1).count())\n",
    ")\n",
    "\n",
    "# Compute the rolling average rating per user over their last 7 reviews\n",
    "reviews_dataset['rolling_rating_mean'] = (\n",
    "    reviews_dataset.groupby('user_id')['rating']\n",
    "    .transform(lambda x: x.rolling(window=7, min_periods=1).mean())\n",
    ")\n",
    "\n",
    "print(\"Rolling averages and count-based features completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb88282d-4bf8-4109-9f7d-3da28ae601df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Behavioral Feature Engineering\n",
    "\n",
    "# Calculate the average rating given by each user\n",
    "user_avg_rating = reviews_dataset.groupby('user_id')['rating'].mean().reset_index(name='avg_rating')\n",
    "\n",
    "# Merge the user-specific average rating back into the main dataset\n",
    "reviews_dataset = reviews_dataset.merge(user_avg_rating, on='user_id', how='left')\n",
    "\n",
    "# Compute the rating deviation: absolute difference between a user's rating and their average rating\n",
    "reviews_dataset['rating_deviation'] = np.abs(reviews_dataset['rating'] - reviews_dataset['avg_rating'])\n",
    "\n",
    "print(\"Behavioral feature engineering completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31376e4-62d8-44fa-a5ba-184ac642dadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group Dynamics Feature Engineering\n",
    "\n",
    "if 'category' in reviews_dataset.columns:\n",
    "    # Construct a graph where nodes represent users and categories, and edges indicate user-category relationships\n",
    "    edges = reviews_dataset[['user_id', 'category']].drop_duplicates().values\n",
    "    review_graph = nx.Graph()\n",
    "    review_graph.add_edges_from(edges)\n",
    "\n",
    "    # Compute degree centrality (measures the importance of a user based on their connections in the network)\n",
    "    degree_centrality = nx.degree_centrality(review_graph)\n",
    "    \n",
    "    # Assign computed degree centrality values to users (default to 0 if not found in the graph)\n",
    "    reviews_dataset['degree_centrality'] = reviews_dataset['user_id'].apply(lambda x: degree_centrality.get(x, 0))\n",
    "else:\n",
    "    print(\"Column 'category' not found. Skipping group dynamics features.\")\n",
    "    \n",
    "    # Assign None to the feature if the category column is missing\n",
    "    reviews_dataset['degree_centrality'] = None\n",
    "\n",
    "print(\"Group dynamics features completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e84b769-a885-4a5c-9e85-67c56540afbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print column names and the number of columns\n",
    "print(f\"Final dataset contains {reviews_dataset.shape[1]} columns.\")\n",
    "print(\"Columns in the final dataset:\", reviews_dataset.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880b2e27-7c14-4686-b1b9-ea59e0ab3a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Final Dataset\n",
    "file_path = 'data/reviews_with_features.csv'\n",
    "reviews_dataset.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Final dataset with engineered features saved successfully to: {file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
